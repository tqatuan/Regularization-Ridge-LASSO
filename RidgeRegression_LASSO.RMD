---
title: 'DSO 530: Orange Juice Case'
author: "Tuan Tran"
date: "11/9/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Task

Build a model that predicts the sales (log of the number of units sold) using ridge and lasso regression models. Select the best one, i.e. the one with the lowest testing MSE.

### Steps

@. Upload the data from `oj.csv` file, call it `oj`. Convert the variable `store` to a categorical factor: `oj$store <- factor(oj$store)`. 

```{r}
#Load data
oj <- read.csv("oj.csv")
#Convert the variable `store` to a categorical factor
oj$store <- factor(oj$store)
attach(oj)
```

@. Specify the model that contains as explanatory variables the logarithm of price and its interaction with linear and quadratic components for feature, brand, and the demographic characteristics of a store’s neighborhood. Price elasticities are most likely affected by demographic characteristics such as the average income of a store’s immediate neighborhood. 

The `model.matrix` statement in `R` allows us to specify the model without having to write out all its terms in detail. The model `y ∼ z ∗ (x1 + x2 + x3)∧2`, for
example, includes the intercept and the following 13 terms: `z, x1, x2, x3, x1 ∗
x2, x1 ∗ x3, x2 ∗ x3, z ∗ x1, z ∗ x2, z ∗ x3, z ∗ x1 ∗ x2, z ∗ x1 ∗ x3, z ∗ x2 ∗ x3`.

Our model, with the three brands represented by two indicator variables, contains 210 factors (including the intercept). This is a very large number, suggesting a shrinkage approach such as Ridge and LASSO for the estimation of its parameters.

Make a model matrix as shown below. Remove the first column.

```{r}
x <- model.matrix(logmove ~ log(price)*(feat + brand 
                   + AGE60 + EDUC + ETHNIC + INCOME + HHLARGE + WORKWOM 
                  + HVAL150 + SSTRDIST + SSTRVOL + CPDIST5 + CPWVOL5)^2,
                 data=oj)
dim(x)
#x[1:3, 1:3]
x<- x[,-1]

```

@. Normalize the variables as they are of very different magnitudes, and we transform them such that each variable has mean 0 and standard deviation 1. Use function `scale()`, specify both `center=` and `scale=` options as `TRUE`

```{r echo = T, results = 'hide'}
scale(x, center=TRUE, scale=TRUE)

```

@. Split the data into training and testing sets. Use `set.seed(1234)` to split the data into training and testing sets.
Let the training set be k=1,000, the rest should go into testing set.


```{r}
set.seed(1234)
k=1000
#Split data into training and testing
train=sample(1:nrow(x), k)
test= -train
#Retrieve testing set from data
y=oj$logmove
y.test=y[test]

```


@. Use the training set to build a ridge regression model with an optimal lambda (use 10-fold cross-validation method to find it). Use the following values of lambda to find an optimal: `grid=10^seq(1,-3,length=10)`. Report the testing MSE of this model.

```{r}
library(glmnet)
library(Matrix)
library(Rcpp)
#Create a vector of possible values of Lambda
grid=10^seq(1,-3,length=10)
#Fit Ridge regression on training set
ridge.mod=glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
plot(ridge.mod)
#perform cross-validation and compute the associated test error
cv.out.ridge <- cv.glmnet(x[train , ], y[train ], alpha = 0)
plot(cv.out.ridge)
#Save the best value of lambda under ridge regression
bestlam.ridge <- cv.out.ridge$lambda.min
#Get predictions for a test set using best lambda value we got from previous command
ridge.pred=predict(ridge.mod, s=bestlam.ridge, newx=x[test, ])
#Compute MSE for the test set
mean((ridge.pred-y.test)^2)

```

@. Repeat the same steps as in previous part to find testing MSS for the lasso regression model.

```{r}
k=1000
#Split data into training and testing
train.ls=sample(1:nrow(x), k)
test.ls<- -train.ls
#Retrieve testing set from data
y.ls=oj$logmove
y.test.ls=y.ls[test.ls]
#Create a vector of possible values of Lambda
grid=10^seq(1,-3,length=10)
#Fit LASSO regression on training set
ls.mod=glmnet(x[train.ls, ], y[train.ls], alpha=1, lambda=grid, thresh=1e-12)
#perform cross-validation and compute the associated test error
cv.out.ls <- cv.glmnet(x[train , ], y[ train ], alpha = 1)
plot (cv.out.ls)
#Save best value of lambda
bestlam.ls <- cv.out.ls$lambda.min
#Get predictions for a test set using the lambda we got from previous command
ls.pred=predict(ls.mod, s=bestlam.ls, newx=x[test.ls, ])
#Compute MSE for the test set
mean((ls.pred-y.test.ls)^2)

```

@. Which model would you choose to predict the orange juice sales?

```{r}
#Retrieve the coefficients of 17 variables under ridge regression
out.ridge <- glmnet(x, y, alpha = 0)
predict(out.ridge , type = "coefficients", s=bestlam.ridge )[1:17, ]
#Retrieve the coefficients of 17 variables under LASSO
out.ls <- glmnet(x,y, alpha = 1)
predict(out.ls, type="coefficients", s=bestlam.ls)[1:17, ]
```

LASSO's MSE is lower than that of Ridge regression methods. 
Besides, we retrieve the value of coefficients in Ridge regression, though the coefficients of some variables approached 0, none of them is equal 0. So Ridge regression doesn't help us to do variable selection.
Meanwhile, LASSO technique returned 12 variables with coefficients value of 0 so it helped us to do variable selection. In short, we should choose LASSO over ridge regression



